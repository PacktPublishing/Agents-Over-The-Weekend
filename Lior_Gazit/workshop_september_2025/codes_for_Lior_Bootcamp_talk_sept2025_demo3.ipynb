{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeCwDW69Zf3"
      },
      "source": [
        "# Demo 3 - Monitoring & Tracing Example, and Model Differences\n",
        "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
        "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)  \n",
        "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.  \n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_sept2025_demo3.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "```\n",
        "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn9qIRLS9Zf7"
      },
      "source": [
        "Installing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMpPyY829Zf9",
        "outputId": "47e0fc2b-abd0-4bc1-eba3-69b29c14b137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip -q install llmpop\n",
        "%pip -q install langchain langchain_core tiktoken langsmith langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uU75SUy9ZgA"
      },
      "source": [
        "**Imports:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVDmCWnO9ZgB",
        "outputId": "faaaec7b-4cc1-4893-986a-595eb3ed5b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded spin_up_LLM.py from GitHub\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWO5Ngf2tDKb"
      },
      "source": [
        "Code example using LangSmiths's trace support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwMTGwwuvgCY",
        "outputId": "82bff6aa-067f-4029-b941-686bd6e4eff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your LangChain API key: ··········\n",
            "Paste your OpenAI API key: ··········\n",
            "\n",
            "Starting tracing for project <multi-agent-demo04>, funtion <multi_agent_interaction>\n",
            "\n",
            "Task for coder to perform:\n",
            "Write a Python function `reverse_string(s)` that returns the reverse of the string.\n",
            "\n",
            "=== Coder Output (Time: 1.02s, Tokens: 41) ===\n",
            "\n",
            "Certainly! Here is a concise Python function to reverse a string:\n",
            "\n",
            "```python\n",
            "def reverse_string(s):\n",
            "    return s[::-1]\n",
            "```\n",
            "\n",
            "This function uses Python's slicing feature to reverse the string.\n",
            "\n",
            "=== Reviewer Feedback (Time: 2.72s, Tokens: 233) ===\n",
            "\n",
            "The provided Python function to reverse a string is already quite efficient and concise. It correctly uses Python's slicing feature to reverse the string. However, there are a few points to consider for improvement or clarification:\n",
            "\n",
            "1. **Type Checking**: Ensure that the input is indeed a string. If the function is called with a non-string type, it might lead to unexpected behavior. You could add a type check to handle such cases gracefully.\n",
            "\n",
            "2. **Docstring**: Adding a docstring can help clarify the purpose and usage of the function.\n",
            "\n",
            "3. **Type Hinting**: Adding type hints can improve code readability and help with static analysis tools.\n",
            "\n",
            "Here's an improved version of the function with these considerations:\n",
            "\n",
            "```python\n",
            "def reverse_string(s: str) -> str:\n",
            "    \"\"\"\n",
            "    Reverses the given string.\n",
            "\n",
            "    Parameters:\n",
            "    s (str): The string to be reversed.\n",
            "\n",
            "    Returns:\n",
            "    str: The reversed string.\n",
            "    \"\"\"\n",
            "    if not isinstance(s, str):\n",
            "        raise TypeError(\"Input must be a string\")\n",
            "    return s[::-1]\n",
            "```\n",
            "\n",
            "These changes ensure that the function is robust, well-documented, and easy to understand.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import openai\n",
        "\n",
        "# Set up environment variables (make sure your keys are set correctly)\n",
        "if \"langchain_api_key\" not in globals():\n",
        "  langchain_api_key = getpass(\"Paste your LangChain API key: \")\n",
        "if not openai.api_key:\n",
        "  openai.api_key = getpass(\"Paste your OpenAI API key: \")\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "# IMPORTANT: If you change the designated project, you must restart the notebook kernel.\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"multi-agent-demo05\"\n",
        "\n",
        "import time\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langsmith import traceable\n",
        "from langsmith.run_helpers import trace\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import tiktoken\n",
        "\n",
        "# Helper to count tokens\n",
        "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
        "    enc = tiktoken.get_encoding(encoding_name)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "# Setup LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Prompts for agents\n",
        "coder_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a coding assistant. Write concise Python code for this task:\\n{task}\"\n",
        ")\n",
        "\n",
        "reviewer_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a meticulous code reviewer. Identify bugs or improvements in the following code:\\n{code}\"\n",
        ")\n",
        "\n",
        "# Chains\n",
        "coder_chain = coder_prompt | llm\n",
        "reviewer_chain = reviewer_prompt | llm\n",
        "\n",
        "# Traceable agent function\n",
        "@traceable(name=\"multi_agent_interaction\")\n",
        "def multi_agent_interaction(task):\n",
        "    # Coder\n",
        "    start_coder = time.time()\n",
        "    coder_response = coder_chain.invoke({\"task\": task})\n",
        "    coder_duration = time.time() - start_coder\n",
        "    coder_code = coder_response.content\n",
        "\n",
        "    print(f\"\\n=== Coder Output (Time: {coder_duration:.2f}s, Tokens: {count_tokens(coder_code)}) ===\\n\")\n",
        "    print(coder_code)\n",
        "\n",
        "    # Reviewer\n",
        "    start_reviewer = time.time()\n",
        "    reviewer_response = reviewer_chain.invoke({\"code\": coder_code})\n",
        "    reviewer_duration = time.time() - start_reviewer\n",
        "    reviewer_feedback = reviewer_response.content\n",
        "\n",
        "    print(f\"\\n=== Reviewer Feedback (Time: {reviewer_duration:.2f}s, Tokens: {count_tokens(reviewer_feedback)}) ===\\n\")\n",
        "    print(reviewer_feedback)\n",
        "\n",
        "# Execute with trace context\n",
        "with trace(\"multi_agent_demo_run\"):\n",
        "    print(\"\\nStarting tracing for project <\" + os.environ[\"LANGCHAIN_PROJECT\"] + \">, funtion <multi_agent_interaction>\")\n",
        "    task_description = \"Write a Python function `reverse_string(s)` that returns the reverse of the string.\"\n",
        "    print(f\"\\nTask for coder to perform:\\n{task_description}\")\n",
        "    multi_agent_interaction(task_description)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F11TLCs39ZgK"
      },
      "source": [
        "Code example for where building the logging process ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7EXGKqf9ZgK",
        "outputId": "9662ff77-5269-48cd-9f54-740a514a918d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "=== Coder’s Code ===\n",
            " \n",
            "[PYTHON]\n",
            "def reverse_string(s):\n",
            "    return s[::-1]\n",
            "[/PYTHON]\n",
            "\n",
            "[TESTS]\n",
            "# Test case 1:\n",
            "assert reverse_string(\"hello\") == \"olleh\"\n",
            "# Test case 2:\n",
            "assert reverse_string(\"\") == \"\"\n",
            "# Test case 3:\n",
            "assert reverse_string(\"a\") == \"a\"\n",
            "# Test case 4:\n",
            "assert reverse_string(\"ab\") == \"ba\"\n",
            "# Test case 5:\n",
            "assert reverse_string(\"abc\") == \"cba\"\n",
            "[/TESTS]\n",
            "\n",
            "\n",
            "=== Reviewer’s Feedback ===\n",
            " \n",
            "This code appears to be correct and well-written. It defines a function `reverse_string` that takes a string as input and returns its reverse. The function is implemented using slicing with step size -1, which is a concise and efficient way to reverse a string in Python.\n",
            "\n",
            "The tests for this code are also good and cover all the edge cases:\n",
            "\n",
            "* Test case 1: The empty string \"\" should be returned unchanged (i.e., it should not be reversed).\n",
            "* Test case 2: A single-character string \"a\" should be returned unchanged (i.e., it should not be reversed).\n",
            "* Test case 3: A two-character string \"ab\" should be reversed to \"ba\".\n",
            "* Test case 4: A three-character string \"abc\" should be reversed to \"cba\".\n",
            "* Test case 5: A longer string with more than two characters should be reversed correctly.\n",
            "\n",
            "The `usage_metadata` dictionary contains information about the usage of this code, such as the number of input and output tokens, total duration, etc.\n",
            "\n",
            "Overall, this code appears to be well-written and efficient, and the tests cover all the edge cases effectively.\n",
            "\n",
            "---\n",
            "Printing the log:\n",
            "{\"step\": \"Coder\", \"prompt_tokens\": 16, \"response_tokens\": 103, \"duration_s\": 9.334, \"timestamp\": 1757390073.3790298}\n",
            "{\"step\": \"Reviewer\", \"prompt_tokens\": 297, \"response_tokens\": 233, \"duration_s\": 6.88, \"timestamp\": 1757390082.71746}\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import logging\n",
        "import json\n",
        "import tiktoken\n",
        "from llmpop import init_llm\n",
        "\n",
        "\n",
        "# 1. Basic logging setup\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
        "\n",
        "# 2. Helper: count tokens using tiktoken\n",
        "def count_tokens(text, encoding_name=\"cl100k_base\"):\n",
        "    enc = tiktoken.get_encoding(encoding_name)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "# 3. Helper: log each call\n",
        "def log_call(step_name, prompt, response, start, end, log_file=\"llm_trace.log\"):\n",
        "    record = {\n",
        "        \"step\": step_name,\n",
        "        \"prompt_tokens\": count_tokens(prompt),\n",
        "        \"response_tokens\": count_tokens(response),\n",
        "        \"duration_s\": round(end - start, 3),\n",
        "        \"timestamp\": start\n",
        "    }\n",
        "    # Console output\n",
        "    logging.info(f\"[{step_name}] {record['duration_s']}s | \"\n",
        "                 f\"prompt_tokens={record['prompt_tokens']} | \"\n",
        "                 f\"response_tokens={record['response_tokens']}\")\n",
        "    # Append to JSON‑lines file\n",
        "    with open(log_file, \"a\") as f:\n",
        "        f.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "# 4. Load your model (local Ollama example)\n",
        "model = init_llm(model=\"CodeLlama\", provider=\"ollama\", verbose=False)\n",
        "\n",
        "# 5. Step 1: Coder agent (generate code)\n",
        "step1_prompt = \"Write a Python function `reverse_string(s)` that returns the reverse of s.\"\n",
        "start = time.time()\n",
        "step1_response = model.invoke(step1_prompt)\n",
        "end = time.time()\n",
        "log_call(\"Coder\", step1_prompt, step1_response.content, start, end)\n",
        "\n",
        "# 6. Step 2: Reviewer agent (review code)\n",
        "step2_prompt = f\"Review this code for correctness and edge cases:\\n\\n{step1_response}\"\n",
        "start = time.time()\n",
        "step2_response = model.invoke(step2_prompt)\n",
        "end = time.time()\n",
        "log_call(\"Reviewer\", step2_prompt, step2_response.content, start, end)\n",
        "\n",
        "# 7. Print outputs\n",
        "print(\"=== Coder’s Code ===\\n\", step1_response.content)\n",
        "print(\"\\n=== Reviewer’s Feedback ===\\n\", step2_response.content)\n",
        "\n",
        "# 8. Inspect the log file if desired:\n",
        "print(\"\\n---\\nPrinting the log:\")\n",
        "!head -n 10 llm_trace.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsmlGaa3AYxt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
