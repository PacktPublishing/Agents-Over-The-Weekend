{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyeCwDW69Zf3"
      },
      "source": [
        "# Demo 4 - Developing a LLM Router with `LLMPop` Using GPT5 for Vibe Coding\n",
        "By: [Lior Gazit](https://github.com/LiorGazit).  \n",
        "Repo: [Agents-Over-The-Weekend](https://github.com/PacktPublishing/Agents-Over-The-Weekend/tree/main/Lior_Gazit/workshop_september_2025/)  \n",
        "Running LLMs locally for free: This code leverages [`LLMPop`](https://pypi.org/project/llmpop/) that is dedicated to spinning up local or remote LLMs in a unified and modular syntax.    \n",
        "\n",
        "In this notebook we experiment vibe-coding with ChatGPT.  \n",
        "We do so by providing GPT5 with a single prompt + reference material so to design a smart LLM router that maps each user prompt to the optimal LLM based on considerations of cost, speed, and depth.  \n",
        "Reference material provided to GPT5 for building the code solution:    \n",
        "1. A machine-readable guide for the library `LLMPop`, a library that unifies call syntax to all LLMs, built to suit LLM routers.  \n",
        "2. A table of LLMs so to provide GPT5 context on various LLMs so to build the routing logic around.  \n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/PacktPublishing/Agents-Over-The-Weekend/blob/main/Lior_Gazit/workshop_september_2025/codes_for_Lior_Bootcamp_talk_demo4.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "```\n",
        "Disclaimer: The content and ideas presented in this notebook are solely those of the author, Lior Gazit, and do not represent the views or intellectual property of the author's employer.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "948vnVL3fCJU"
      },
      "source": [
        "## Prompt for coder-LLM:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfuxtDbzfjQb"
      },
      "source": [
        "\n",
        "Write a Python script (Later to become a stand alone Jupyter notebook) that demonstrates a small-scale LLM router using the LLMPop library.\n",
        "\n",
        "For information on LLMPop:  \n",
        "Details on the LLMPop library are described in the attached document: LLM_READABLE_GUIDE.md  \n",
        "Refere to LLM_READABLE_GUIDE.md for understanding how to use the LLMPop library.  \n",
        "\n",
        "For information on the different models available on Ollama:  \n",
        "Refer to the file Ollama models by size and call name (a limited list).csv  \n",
        "It has a limited list of models, and it gives some idea on which are lighted and which are heavier in size.  \n",
        "\n",
        "### Requirements:\n",
        "\n",
        "1. Purpose: The script routes user prompts to one of several available LLMs (some from Ollama for local inference, others from OpenAI’s API, per the LLMPop libraries functionality) based on simple routing logic.\n",
        "2. Inputs to Routing Function:\n",
        "   * User prompt (text)\n",
        "   * Optional conversation context (previous messages, user metadata, etc.)\n",
        "   * Optional optimization criteria (e.g., prioritize cost, speed, or reasoning depth)\n",
        "   * Optional runtime resource information (e.g., system load, API quotas)\n",
        "3. Routing Logic: Implement a basic, illustrative strategy to decide which model to call.  \n",
        "   For example:\n",
        "   * Short/simple prompts → lightweight, fast model (local Ollama model).\n",
        "   * Complex prompts (math/logic/code) → reasoning-heavy OpenAI GPT-4 model.\n",
        "   * Requests flagged as “specialized” → a chosen domain-specific model.\n",
        "   * Add dummy evaluation for resource optimization (simulate cost or latency).\n",
        "4. Output: Show which model is selected, call it with a test prompt, and print a sample response.\n",
        "5. Implementation Notes:\n",
        "   * Import and use functions/classes from LLMPop (assume the library is installed and documentation is available).\n",
        "   * Include a main() function that:\n",
        "     * Accepts a sample user query,\n",
        "     * Runs it through your routing logic,\n",
        "     * Dispatches it to the chosen LLM.\n",
        "   * Add inline comments explaining the decision-making process.\n",
        "   * Keep it minimal, clean, and easy to adapt as a demo or template.\n",
        "\n",
        "### Output Expectations:\n",
        "* Full runnable Python code (later to be a notebook).\n",
        "* Clear, commented code that demonstrates basic extensibility for future routing improvements.\n",
        "* Use resource checks.\n",
        "\n",
        "### Goal:\n",
        "Deliver a concise, readable prototype demonstrating LLM routing patterns, not a production service."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Akit5rDfOtR"
      },
      "source": [
        "## The perliminary code the bot wrote:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idfDz4Bufnzp"
      },
      "source": [
        "```python\n",
        "\n",
        "\"\"\"\n",
        "Small-Scale LLM Router Demo\n",
        "---------------------------\n",
        "Purpose:\n",
        "  Route a user prompt to a local (Ollama) or remote (OpenAI) model using basic,\n",
        "  illustrative heuristics. Keep it readable and easy to adapt.\n",
        "\n",
        "Prereqs:\n",
        "  pip -q install llmpop\n",
        "  pip install langchain_core psutil pandas\n",
        "\n",
        "Notes:\n",
        "  - Reads an optional CSV of Ollama models to choose a lightweight or heavier local model:\n",
        "      /mnt/data/Ollama models by size and call name (a limited list).csv\n",
        "  - Uses `init_llm` from LLMPop (returns a LangChain ChatModel).\n",
        "  - Uses simple heuristics to choose a model: complexity, specialization, optimization,\n",
        "    plus a quick CPU/memory snapshot and dummy cost/latency scoring.\n",
        "  - Falls back gracefully if a model or provider isn't available.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "# --- LLMPop imports (per your guide) ---\n",
        "# Guide reference: init_llm() returns a ChatModel (ChatOllama / ChatOpenAI)\n",
        "# and start_resource_monitoring() optionally logs CPU/Mem/GPU in a background thread.\n",
        "# (This demo just shows you how to call them; monitoring is optional.)\n",
        "from llmpop import init_llm, start_resource_monitoring  # :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "# --- Optional utilities ---\n",
        "import psutil\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------\n",
        "# Model catalog & metadata\n",
        "# -----------------------\n",
        "\n",
        "@dataclass\n",
        "class ModelSpec:\n",
        "    name: str               # provider-specific model id (e.g., 'llama3:8b' or 'gpt-4o-mini')\n",
        "    provider: str           # 'ollama' or 'openai'\n",
        "    size_rank: int          # smaller number = lighter/faster (heuristic)\n",
        "    tags: List[str]         # ['general', 'code', 'math', 'reasoning', 'specialized:finance', ...]\n",
        "    est_cost_per_1k: float  # dummy USD per 1k tokens (for routing illustration)\n",
        "    est_latency: float      # dummy seconds base latency (for routing illustration)\n",
        "    notes: str = \"\"\n",
        "\n",
        "def _read_ollama_csv(csv_path: str) -> List[ModelSpec]:\n",
        "    \"\"\"\n",
        "    Read the user's 'Ollama models by size and call name (a limited list).csv'\n",
        "    and convert it into ModelSpec entries with rough size ranks.\n",
        "    The CSV is expected to have columns like: 'call_name', 'size' (text), etc.\n",
        "    This function is defensive and will skip rows missing these fields.\n",
        "    \"\"\"\n",
        "    specs: List[ModelSpec] = []\n",
        "    if not os.path.exists(csv_path):\n",
        "        return specs\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception:\n",
        "        return specs\n",
        "\n",
        "    # Try to infer a size rank from a 'size' or 'parameters' column\n",
        "    def infer_size_rank(size_str: str) -> int:\n",
        "        if not isinstance(size_str, str):\n",
        "            return 5\n",
        "        s = size_str.lower()\n",
        "        # simple heuristics: lower rank is \"lighter\"\n",
        "        if any(k in s for k in [\"2b\", \"3b\", \"tiny\", \"mini\", \"small\"]): return 1\n",
        "        if any(k in s for k in [\"7b\", \"8b\", \"medium\"]): return 2\n",
        "        if any(k in s for k in [\"13b\", \"14b\", \"large\"]): return 3\n",
        "        if any(k in s for k in [\"30b\", \"34b\", \"xl\"]): return 4\n",
        "        return 5\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        call_name = str(row.get(\"call_name\") or row.get(\"name\") or \"\").strip()\n",
        "        size_str  = str(row.get(\"size\") or row.get(\"parameters\") or \"\").strip()\n",
        "        if not call_name:\n",
        "            continue\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=call_name,\n",
        "                provider=\"ollama\",\n",
        "                size_rank=infer_size_rank(size_str),\n",
        "                tags=[\"general\"],\n",
        "                est_cost_per_1k=0.0,        # local inference → treat as $0 for demo\n",
        "                est_latency=max(0.2, 0.05 * infer_size_rank(size_str)),\n",
        "                notes=f\"From CSV; size='{size_str}'\"\n",
        "            )\n",
        "        )\n",
        "    return specs\n",
        "\n",
        "def _default_catalog() -> List[ModelSpec]:\n",
        "    \"\"\"\n",
        "    Default demo catalog if CSV is unavailable. Adjust to your environment.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        # Local (Ollama) — fast/light general chat\n",
        "        ModelSpec(name=\"llama3.1:8b-instruct\", provider=\"ollama\", size_rank=2,\n",
        "                  tags=[\"general\"], est_cost_per_1k=0.0, est_latency=0.3,\n",
        "                  notes=\"Local, lightweight general model\"),\n",
        "        # Local (Ollama) — code-aware (replace with your local favorite)\n",
        "        ModelSpec(name=\"codellama:7b-instruct\", provider=\"ollama\", size_rank=2,\n",
        "                  tags=[\"code\"], est_cost_per_1k=0.0, est_latency=0.35,\n",
        "                  notes=\"Local, code-oriented\"),\n",
        "        # Remote (OpenAI) — reasoning-heavy\n",
        "        ModelSpec(name=\"gpt-4o\", provider=\"openai\", size_rank=4,\n",
        "                  tags=[\"reasoning\", \"math\", \"complex\"], est_cost_per_1k=5.0, est_latency=0.6,\n",
        "                  notes=\"Remote, strong reasoning\"),\n",
        "        # Remote (OpenAI) — fast/cheap\n",
        "        ModelSpec(name=\"gpt-4o-mini\", provider=\"openai\", size_rank=1,\n",
        "                  tags=[\"general\", \"fast\"], est_cost_per_1k=0.6, est_latency=0.25,\n",
        "                  notes=\"Remote, budget/latency friendly\"),\n",
        "    ]\n",
        "\n",
        "def build_catalog(csv_path: str) -> List[ModelSpec]:\n",
        "    specs = _read_ollama_csv(csv_path)\n",
        "    specs += _default_catalog()\n",
        "    return specs\n",
        "\n",
        "# -----------------------\n",
        "# Lightweight heuristics\n",
        "# -----------------------\n",
        "\n",
        "def is_complex_prompt(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Naive complexity signal: math markers, code fences, long length, or\n",
        "    words indicating step-by-step reasoning.\n",
        "    \"\"\"\n",
        "    if len(text) > 300:\n",
        "        return True\n",
        "    code_markers = [\"```\", \"def \", \"class \", \"SELECT \", \"FROM \", \"import \", \"public static void\"]\n",
        "    math_markers = [r\"\\bprove\\b\", r\"\\bderive\\b\", r\"\\bO\\(\", r\"\\bintegral\\b\", r\"[0-9]+\\s*[\\+\\-\\*\\/\\^]\\s*[0-9]+\"]\n",
        "    reasoning_terms = [r\"\\bchain of thought\\b\", r\"\\bstep-by-step\\b\", r\"\\bthink step by step\\b\"]\n",
        "\n",
        "    if any(m in text for m in code_markers):\n",
        "        return True\n",
        "    if any(re.search(p, text, flags=re.IGNORECASE) for p in math_markers + reasoning_terms):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def detect_specialization(text: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Return a specialization tag when obvious (very naive).\n",
        "    \"\"\"\n",
        "    pairs = {\n",
        "        \"code\": [r\"\\bwrite (a|the) (python|js|sql|java|c\\+\\+|c#)\"],\n",
        "        \"sql\":  [r\"\\bSELECT\\b.*\\bFROM\\b\", r\"\\bCREATE TABLE\\b\"],\n",
        "        \"finance\": [r\"\\bDCF\\b\", r\"\\bWACC\\b\", r\"\\b10-K\\b\"],\n",
        "        \"biology\": [r\"\\bRNA\\b\", r\"\\bCRISPR\\b\"],\n",
        "        \"law\": [r\"\\bstatute\\b\", r\"\\btort\\b\"],\n",
        "        \"math\": [r\"\\bprove\\b\", r\"\\bintegral\\b\", r\"\\bderivative\\b\"],\n",
        "    }\n",
        "    for tag, patterns in pairs.items():\n",
        "        if any(re.search(p, text, flags=re.IGNORECASE | re.DOTALL) for p in patterns):\n",
        "            return tag\n",
        "    return None\n",
        "\n",
        "def snapshot_resources() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Quick resource snapshot (CPU %, available RAM GB). These values can inform routing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cpu = psutil.cpu_percent(interval=0.2)\n",
        "        mem = psutil.virtual_memory()\n",
        "        avail_gb = round(mem.available / (1024**3), 2)\n",
        "        return {\"cpu_percent\": cpu, \"available_gb\": avail_gb}\n",
        "    except Exception:\n",
        "        return {\"cpu_percent\": None, \"available_gb\": None}\n",
        "\n",
        "def score_candidate(\n",
        "    spec: ModelSpec,\n",
        "    *,\n",
        "    optimization: str,\n",
        "    complex_prompt: bool,\n",
        "    specialization: Optional[str],\n",
        "    resources: Dict[str, Any]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Lower score = better. Combine a few simple signals:\n",
        "      - If complex → prefer 'reasoning' models.\n",
        "      - If specialization detected → prefer related tags (e.g., 'code', 'math').\n",
        "      - Optimization: 'speed', 'cost', or 'depth' (reasoning depth).\n",
        "      - If CPU is high and provider == 'ollama' → small penalty (local load).\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "\n",
        "    # Base on optimization goal\n",
        "    if optimization == \"speed\":\n",
        "        score += spec.est_latency * 2.0 + spec.size_rank * 0.5\n",
        "    elif optimization == \"cost\":\n",
        "        score += spec.est_cost_per_1k * 1.5 + spec.size_rank * 0.3\n",
        "    elif optimization == \"depth\":\n",
        "        # prefer reasoning-heavy\n",
        "        score += (0 if (\"reasoning\" in spec.tags or \"complex\" in spec.tags) else 1.5)\n",
        "        score += spec.size_rank * 0.2\n",
        "    else:\n",
        "        # default blend\n",
        "        score += spec.est_latency + 0.2 * spec.est_cost_per_1k\n",
        "\n",
        "    # Complexity\n",
        "    if complex_prompt and not any(t in spec.tags for t in [\"reasoning\", \"complex\", \"math\", \"code\"]):\n",
        "        score += 1.0\n",
        "\n",
        "    # Specialization\n",
        "    if specialization:\n",
        "        if specialization in spec.tags or f\"specialized:{specialization}\" in spec.tags:\n",
        "            score -= 0.5  # small bonus\n",
        "        else:\n",
        "            # mild penalty if it doesn't look appropriate\n",
        "            score += 0.3\n",
        "\n",
        "    # Resource-aware: if CPU is busy and we're local, prefer remote\n",
        "    cpu = resources.get(\"cpu_percent\")\n",
        "    if cpu is not None and cpu > 70 and spec.provider == \"ollama\":\n",
        "        score += 0.8\n",
        "\n",
        "    # If RAM is very low and spec is large local model (size_rank>=3), penalize\n",
        "    avail_gb = resources.get(\"available_gb\")\n",
        "    if avail_gb is not None and avail_gb < 2.0 and spec.provider == \"ollama\" and spec.size_rank >= 3:\n",
        "        score += 0.7\n",
        "\n",
        "    return score\n",
        "\n",
        "# -----------------------\n",
        "# Dispatch\n",
        "# -----------------------\n",
        "\n",
        "def init_model(spec: ModelSpec):\n",
        "    \"\"\"\n",
        "    Initialize a ChatModel via LLMPop's init_llm().\n",
        "    For Ollama, auto_serve/auto_install/pull are enabled to make the demo smoother.\n",
        "    For OpenAI, use env OPENAI_API_KEY or pass provider_kwargs with api_key.\n",
        "    \"\"\"\n",
        "    if spec.provider == \"ollama\":\n",
        "        return init_llm(\n",
        "            model=spec.name,\n",
        "            provider=\"ollama\",\n",
        "            provider_kwargs={\n",
        "                \"auto_install\": True,\n",
        "                \"auto_serve\": True,\n",
        "                \"pull\": True,\n",
        "            },\n",
        "            temperature=0.0,\n",
        "        )  # :contentReference[oaicite:2]{index=2}\n",
        "\n",
        "    elif spec.provider == \"openai\":\n",
        "        # You can also pass provider_kwargs={\"api_key\": \"...\"} (guide allows both env or kwarg)\n",
        "        return init_llm(\n",
        "            model=spec.name,\n",
        "            provider=\"openai\",\n",
        "            temperature=0.0,\n",
        "        )  # :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {spec.provider}\")\n",
        "\n",
        "def call_model(chat_model, user_prompt: str, system_msg: str = \"You are a helpful assistant.\"):\n",
        "    \"\"\"\n",
        "    Call a LangChain ChatModel returned by init_llm(). Keep it simple with a short system + human turn.\n",
        "    \"\"\"\n",
        "    from langchain_core.messages import SystemMessage, HumanMessage\n",
        "    # Return raw content; for pure string you could add StrOutputParser (see guide). :contentReference[oaicite:4]{index=4}\n",
        "    result = chat_model.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
        "    return getattr(result, \"content\", str(result))\n",
        "\n",
        "# -----------------------\n",
        "# Main router\n",
        "# -----------------------\n",
        "\n",
        "def route_and_dispatch(\n",
        "    prompt: str,\n",
        "    *,\n",
        "    conversation_ctx: Optional[List[Dict[str, Any]]] = None,\n",
        "    optimization: str = \"speed\",   # 'speed' | 'cost' | 'depth'\n",
        "    runtime_info: Optional[Dict[str, Any]] = None,\n",
        "    csv_path: str = \"/mnt/data/Ollama models by size and call name (a limited list).csv\",\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Core router: choose a model and execute the call. Returns a dict with\n",
        "    selection details and the model output.\n",
        "    \"\"\"\n",
        "    catalog = build_catalog(csv_path)\n",
        "\n",
        "    complex_prompt = is_complex_prompt(prompt)\n",
        "    specialization = detect_specialization(prompt)\n",
        "    resources = snapshot_resources()\n",
        "\n",
        "    # Incorporate any runtime quotas into a tiny penalty (dummy)\n",
        "    quotas = (runtime_info or {}).get(\"api_quotas\", {})\n",
        "    openai_quota_low = quotas.get(\"openai_remaining\", 1000) < 50\n",
        "    ollama_quota_low = quotas.get(\"ollama_remaining\", 1000) < 10\n",
        "\n",
        "    # Score each candidate\n",
        "    scored = []\n",
        "    for spec in catalog:\n",
        "        s = score_candidate(spec,\n",
        "                            optimization=optimization,\n",
        "                            complex_prompt=complex_prompt,\n",
        "                            specialization=specialization,\n",
        "                            resources=resources)\n",
        "        # Quotas as penalties (illustrative)\n",
        "        if spec.provider == \"openai\" and openai_quota_low:\n",
        "            s += 1.0\n",
        "        if spec.provider == \"ollama\" and ollama_quota_low:\n",
        "            s += 0.5\n",
        "        scored.append((s, spec))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0])\n",
        "    chosen = scored[0][1]\n",
        "\n",
        "    # Try to init and call; fall back if needed\n",
        "    error = None\n",
        "    response = None\n",
        "    try:\n",
        "        chat_model = init_model(chosen)\n",
        "        response = call_model(chat_model, prompt)\n",
        "    except Exception as e:\n",
        "        error = f\"{type(e).__name__}: {e}\"\n",
        "        # Fallback: attempt the next candidate\n",
        "        if len(scored) > 1:\n",
        "            try:\n",
        "                alt = scored[1][1]\n",
        "                chat_model = init_model(alt)\n",
        "                response = call_model(chat_model, prompt)\n",
        "                chosen = alt\n",
        "            except Exception as e2:\n",
        "                response = f\"[Router demo fallback] Could not reach any LLM. Last error: {type(e2).__name__}: {e2}\"\n",
        "\n",
        "    return {\n",
        "        \"chosen_model\": chosen.name,\n",
        "        \"provider\": chosen.provider,\n",
        "        \"notes\": chosen.notes,\n",
        "        \"complex_prompt\": complex_prompt,\n",
        "        \"specialization\": specialization,\n",
        "        \"optimization\": optimization,\n",
        "        \"resources\": resources,\n",
        "        \"error\": error,\n",
        "        \"response\": response,\n",
        "    }\n",
        "\n",
        "# -----------------------\n",
        "# Demo entrypoint\n",
        "# -----------------------\n",
        "\n",
        "def main():\n",
        "    # Optional: start a background resource logger (1 minute, every 5s) — for demo visibility.\n",
        "    # This is purely illustrative; comment out if not desired.\n",
        "    try:\n",
        "        _ = start_resource_monitoring(logfile=\"resource_usage.log\", duration=60, interval=5)  # :contentReference[oaicite:5]{index=5}\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Sample inputs (edit these to experiment)\n",
        "    sample_prompt = (\n",
        "        \"Write a short Python function that returns the nth Fibonacci number, \"\n",
        "        \"then analyze the time complexity.\"\n",
        "    )\n",
        "    conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
        "    optimization = \"depth\"  # try 'speed', 'cost', or 'depth'\n",
        "\n",
        "    # Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
        "    runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
        "\n",
        "    result = route_and_dispatch(\n",
        "        prompt=sample_prompt,\n",
        "        conversation_ctx=conversation_ctx,\n",
        "        optimization=optimization,\n",
        "        runtime_info=runtime_info,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Router Decision ===\")\n",
        "    print(f\"Chosen: {result['chosen_model']} (provider={result['provider']})\")\n",
        "    print(f\"Notes:  {result['notes']}\")\n",
        "    print(f\"Complex prompt? {result['complex_prompt']}; specialization: {result['specialization']}\")\n",
        "    print(f\"Optimization: {result['optimization']}\")\n",
        "    print(f\"Resources snapshot: {result['resources']}\")\n",
        "    if result[\"error\"]:\n",
        "        print(f\"Init error (first try): {result['error']}\")\n",
        "\n",
        "    print(\"\\n=== Model Output (truncated) ===\")\n",
        "    print(str(result[\"response\"])[:1200])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deKBVzaiJ6ZY"
      },
      "source": [
        "## The Actual Resulting Code\n",
        "I took the code that the bot generated after a single prompt, and adjusted it to suit my needs.  \n",
        "All adjustments were minor.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfNq2s9Ufv57"
      },
      "source": [
        "The bot told about the code it designed:  \n",
        "\n",
        "**Small-Scale LLM Router Demo**  \n",
        "Purpose:  \n",
        "  Route a user prompt to a local (Ollama) or remote (OpenAI) model using basic,\n",
        "  illustrative heuristics. Keep it readable and easy to adapt.  \n",
        "\n",
        "Prereqs:  \n",
        "`pip -q install llmpop `  \n",
        "`pip install langchain_core psutil pandas`  \n",
        "\n",
        "Notes:  \n",
        "  - Reads an optional CSV of Ollama models to choose a lightweight or heavier local model:  \n",
        "      `Ollama models by size and call name (a limited list).csv`  \n",
        "  - Uses `init_llm` from LLMPop (returns a LangChain ChatModel).  \n",
        "  - Uses simple heuristics to choose a model: complexity, specialization, optimization,\n",
        "    plus a quick CPU/memory snapshot and dummy cost/latency scoring.  \n",
        "  - Falls back gracefully if a model or provider isn't available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haMbL9fiK4vY"
      },
      "source": [
        "**Installations:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "icke82ozgLsE"
      },
      "outputs": [],
      "source": [
        "%pip -q install llmpop\n",
        "%pip -q install langchain_core psutil pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEFEfTYRK8-p"
      },
      "source": [
        "**Imports:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P2LJUSLogQGy"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional\n",
        "from getpass import getpass\n",
        "\n",
        "# --- LLMPop imports (per LLM_READABLE_GUIDE.md) ---\n",
        "# Guide reference: init_llm() returns a ChatModel (ChatOllama / ChatOpenAI)\n",
        "# and start_resource_monitoring() optionally logs CPU/Mem/GPU in a background thread.\n",
        "from llmpop import init_llm, start_resource_monitoring  # :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "# --- Optional utilities ---\n",
        "import psutil\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjUMNrMsMGNx"
      },
      "source": [
        "**Settings:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rQZi-P0HMG4n"
      },
      "outputs": [],
      "source": [
        "allow_paid_models = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aug3DrkL7u-"
      },
      "source": [
        "**Insert your OpenAI API key:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3kXhw3JhLK2",
        "outputId": "6e23a3c3-50a0-4626-fe3f-43e5ac987042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "if allow_paid_models:\n",
        "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rztR3Endgav6"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Model catalog & metadata\n",
        "# -----------------------\n",
        "\n",
        "@dataclass\n",
        "class ModelSpec:\n",
        "    name: str               # provider-specific model id (e.g., 'llama3:8b' or 'gpt-4o-mini')\n",
        "    provider: str           # 'ollama' or 'openai'\n",
        "    size_rank: int          # smaller number = lighter/faster (heuristic)\n",
        "    tags: List[str]         # ['general', 'code', 'math', 'reasoning', 'specialized:finance', ...]\n",
        "    est_cost_per_1k: float  # dummy USD per 1k tokens (for routing illustration)\n",
        "    est_latency: float      # dummy seconds base latency (for routing illustration)\n",
        "    notes: str = \"\"\n",
        "\n",
        "def _read_ollama_csv(csv_path: str) -> List[ModelSpec]:\n",
        "    \"\"\"\n",
        "    Read the user's 'Ollama models by size and call name (a limited list).csv'\n",
        "    and convert it into ModelSpec entries with rough size ranks.\n",
        "    The CSV is expected to have columns like: 'call_name', 'size' (text), etc.\n",
        "    This function is defensive and will skip rows missing these fields.\n",
        "    \"\"\"\n",
        "    specs: List[ModelSpec] = []\n",
        "    if not os.path.exists(csv_path):\n",
        "        return specs\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "    except Exception:\n",
        "        return specs\n",
        "\n",
        "    # Try to infer a size rank from a 'size' or 'parameters' column\n",
        "    def infer_size_rank(size_str: str) -> int:\n",
        "        if not isinstance(size_str, str):\n",
        "            return 5\n",
        "        s = size_str.lower()\n",
        "        # simple heuristics: lower rank is \"lighter\"\n",
        "        if any(k in s for k in [\"2b\", \"3b\", \"tiny\", \"mini\", \"small\"]): return 1\n",
        "        if any(k in s for k in [\"7b\", \"8b\", \"medium\"]): return 2\n",
        "        if any(k in s for k in [\"13b\", \"14b\", \"large\"]): return 3\n",
        "        if any(k in s for k in [\"30b\", \"34b\", \"xl\"]): return 4\n",
        "        return 5\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        call_name = str(row.get(\"call_name\") or row.get(\"name\") or \"\").strip()\n",
        "        size_str  = str(row.get(\"size\") or row.get(\"parameters\") or \"\").strip()\n",
        "        if not call_name:\n",
        "            continue\n",
        "        specs.append(\n",
        "            ModelSpec(\n",
        "                name=call_name,\n",
        "                provider=\"ollama\",\n",
        "                size_rank=infer_size_rank(size_str),\n",
        "                tags=[\"general\"],\n",
        "                est_cost_per_1k=0.0,        # local inference → treat as $0 for demo\n",
        "                est_latency=max(0.2, 0.05 * infer_size_rank(size_str)),\n",
        "                notes=f\"From CSV; size='{size_str}'\"\n",
        "            )\n",
        "        )\n",
        "    return specs\n",
        "\n",
        "def _default_catalog(allow_paid_models: bool = True) -> List[ModelSpec]:\n",
        "    \"\"\"\n",
        "    Default demo catalog if CSV is unavailable. Adjust to your environment.\n",
        "    \"\"\"\n",
        "    catalog = [\n",
        "        # Local (Ollama) — fast/light general chat\n",
        "        ModelSpec(name=\"llama3.2:1b\", provider=\"ollama\", size_rank=2,\n",
        "                  tags=[\"general\"], est_cost_per_1k=0.0, est_latency=0.3,\n",
        "                  notes=\"Local, lightweight general model\"),\n",
        "        # Local (Ollama) — code-aware (replace with your local favorite)\n",
        "        ModelSpec(name=\"codellama\", provider=\"ollama\", size_rank=2,\n",
        "                  tags=[\"code\"], est_cost_per_1k=0.0, est_latency=0.35,\n",
        "                  notes=\"Local, code-oriented\"),\n",
        "        # Local (Ollama) — medium general chat\n",
        "        ModelSpec(name=\"gpt-oss:20b\", provider=\"ollama\", size_rank=3,\n",
        "                  tags=[\"general\", \"code\", \"math\", \"complex\"], est_cost_per_1k=0.0, est_latency=0.5,\n",
        "                  notes=\"Local, medium size general model\")]\n",
        "    if allow_paid_models:\n",
        "      catalog += [\n",
        "          # Remote (OpenAI) — reasoning-heavy\n",
        "          ModelSpec(name=\"gpt-4o-mini\", provider=\"openai\", size_rank=1,\n",
        "                    tags=[\"general\", \"fast\"], est_cost_per_1k=0.6, est_latency=0.8,\n",
        "                    notes=\"Remote, budget/latency friendly\"),\n",
        "          # Remote (OpenAI) — fast/cheap\n",
        "          ModelSpec(name=\"gpt-4o\", provider=\"openai\", size_rank=4,\n",
        "                    tags=[\"reasoning\", \"math\", \"complex\"], est_cost_per_1k=5.0, est_latency=0.1,\n",
        "                    notes=\"Remote, strong reasoning\")\n",
        "          ]\n",
        "    return catalog\n",
        "\n",
        "def build_catalog(csv_path: str, allow_paid_models: bool = True) -> List[ModelSpec]:\n",
        "    specs = _read_ollama_csv(csv_path)\n",
        "    specs += _default_catalog(allow_paid_models)\n",
        "    return specs\n",
        "\n",
        "# -----------------------\n",
        "# Lightweight heuristics\n",
        "# -----------------------\n",
        "\n",
        "def is_complex_prompt(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Naive complexity signal: math markers, code fences, long length, or\n",
        "    words indicating step-by-step reasoning.\n",
        "    \"\"\"\n",
        "    if len(text) > 300:\n",
        "        return True\n",
        "    code_markers = [\"```\", \"def \", \"class \", \"SELECT \", \"FROM \", \"import \", \"public static void\"]\n",
        "    math_markers = [r\"\\bprove\\b\", r\"\\bderive\\b\", r\"\\bO\\(\", r\"\\bintegral\\b\", r\"[0-9]+\\s*[\\+\\-\\*\\/\\^]\\s*[0-9]+\"]\n",
        "    reasoning_terms = [r\"\\bchain of thought\\b\", r\"\\bstep-by-step\\b\", r\"\\bthink step by step\\b\"]\n",
        "\n",
        "    if any(m in text for m in code_markers):\n",
        "        return True\n",
        "    if any(re.search(p, text, flags=re.IGNORECASE) for p in math_markers + reasoning_terms):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def detect_specialization(text: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Return a specialization tag when obvious (very naive).\n",
        "    \"\"\"\n",
        "    pairs = {\n",
        "        \"code\": [r\"\\bwrite (a|the) (python|js|sql|java|c\\+\\+|c#)\"],\n",
        "        \"sql\":  [r\"\\bSELECT\\b.*\\bFROM\\b\", r\"\\bCREATE TABLE\\b\"],\n",
        "        \"finance\": [r\"\\bDCF\\b\", r\"\\bWACC\\b\", r\"\\b10-K\\b\"],\n",
        "        \"biology\": [r\"\\bRNA\\b\", r\"\\bCRISPR\\b\"],\n",
        "        \"law\": [r\"\\bstatute\\b\", r\"\\btort\\b\"],\n",
        "        \"math\": [r\"\\bprove\\b\", r\"\\bintegral\\b\", r\"\\bderivative\\b\"],\n",
        "    }\n",
        "    for tag, patterns in pairs.items():\n",
        "        if any(re.search(p, text, flags=re.IGNORECASE | re.DOTALL) for p in patterns):\n",
        "            return tag\n",
        "    return None\n",
        "\n",
        "def snapshot_resources() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Quick resource snapshot (CPU %, available RAM GB). These values can inform routing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cpu = psutil.cpu_percent(interval=0.2)\n",
        "        mem = psutil.virtual_memory()\n",
        "        avail_gb = round(mem.available / (1024**3), 2)\n",
        "        return {\"cpu_percent\": cpu, \"available_gb\": avail_gb}\n",
        "    except Exception:\n",
        "        return {\"cpu_percent\": None, \"available_gb\": None}\n",
        "\n",
        "def score_candidate(\n",
        "    spec: ModelSpec,\n",
        "    *,\n",
        "    optimization: str,\n",
        "    complex_prompt: bool,\n",
        "    specialization: Optional[str],\n",
        "    resources: Dict[str, Any]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Lower score = better. Combine a few simple signals:\n",
        "      - If complex → prefer 'reasoning' models.\n",
        "      - If specialization detected → prefer related tags (e.g., 'code', 'math').\n",
        "      - Optimization: 'speed', 'cost', or 'depth' (reasoning depth).\n",
        "      - If CPU is high and provider == 'ollama' → small penalty (local load).\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "\n",
        "    # Based on optimization goal\n",
        "    if optimization == \"speed\":\n",
        "        score += spec.est_latency * 2.0 + spec.size_rank * 0.5\n",
        "    elif optimization == \"cost\":\n",
        "        score += spec.est_cost_per_1k * 1.5 + spec.size_rank * 0.3\n",
        "    elif optimization == \"depth\":\n",
        "        # prefer reasoning-heavy\n",
        "        score += (0 if (\"reasoning\" in spec.tags or \"complex\" in spec.tags) else 1.5)\n",
        "        score += spec.size_rank * 0.2\n",
        "    else:\n",
        "        # default blend\n",
        "        score += spec.est_latency + 0.2 * spec.est_cost_per_1k\n",
        "\n",
        "    # Complexity\n",
        "    if complex_prompt and not any(t in spec.tags for t in [\"reasoning\", \"complex\", \"math\", \"code\"]):\n",
        "        score += 1.0\n",
        "\n",
        "    # Specialization\n",
        "    if specialization:\n",
        "        if specialization in spec.tags or f\"specialized:{specialization}\" in spec.tags:\n",
        "            score -= 0.5  # small bonus\n",
        "        else:\n",
        "            # mild penalty if it doesn't look appropriate\n",
        "            score += 0.3\n",
        "\n",
        "    # Resource-aware: if CPU is busy and we're local, prefer remote\n",
        "    cpu = resources.get(\"cpu_percent\")\n",
        "    if cpu is not None and cpu > 70 and spec.provider == \"ollama\":\n",
        "        score += 0.8\n",
        "\n",
        "    # If RAM is very low and spec is large local model (size_rank>=3), penalize\n",
        "    avail_gb = resources.get(\"available_gb\")\n",
        "    if avail_gb is not None and avail_gb < 2.0 and spec.provider == \"ollama\" and spec.size_rank >= 3:\n",
        "        score += 0.7\n",
        "\n",
        "    return score\n",
        "\n",
        "# -----------------------\n",
        "# Dispatch\n",
        "# -----------------------\n",
        "\n",
        "def init_model(spec: ModelSpec):\n",
        "    \"\"\"\n",
        "    Initialize a ChatModel via LLMPop's init_llm().\n",
        "    For Ollama, auto_serve/auto_install/pull are enabled to make the demo smoother.\n",
        "    For OpenAI, use env OPENAI_API_KEY or pass provider_kwargs with api_key.\n",
        "    \"\"\"\n",
        "    if spec.provider == \"ollama\":\n",
        "        return init_llm(\n",
        "            model=spec.name,\n",
        "            provider=\"ollama\",\n",
        "            provider_kwargs={\n",
        "                \"auto_install\": True,\n",
        "                \"auto_serve\": True,\n",
        "                \"pull\": True,\n",
        "            },\n",
        "            temperature=0.0,\n",
        "        )  # :contentReference[oaicite:2]{index=2}\n",
        "\n",
        "    elif spec.provider == \"openai\":\n",
        "        return init_llm(\n",
        "            model=spec.name,\n",
        "            provider=\"openai\",\n",
        "            temperature=0.0,\n",
        "        )  # :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown provider: {spec.provider}\")\n",
        "\n",
        "def call_model(chat_model, user_prompt: str, system_msg: str = \"You are a helpful assistant.\"):\n",
        "    \"\"\"\n",
        "    Call a LangChain ChatModel returned by init_llm(). Keep it simple with a short system + human turn.\n",
        "    \"\"\"\n",
        "    from langchain_core.messages import SystemMessage, HumanMessage\n",
        "    # Return raw content; for pure string you could add StrOutputParser (see guide). :contentReference[oaicite:4]{index=4}\n",
        "    result = chat_model.invoke([SystemMessage(content=system_msg), HumanMessage(content=user_prompt)])\n",
        "    return getattr(result, \"content\", str(result))\n",
        "\n",
        "# -----------------------\n",
        "# Main router\n",
        "# -----------------------\n",
        "\n",
        "def route_and_dispatch(\n",
        "    prompt: str,\n",
        "    *,\n",
        "    conversation_ctx: Optional[List[Dict[str, Any]]] = None,\n",
        "    optimization: str = \"speed\",   # 'speed' | 'cost' | 'depth'\n",
        "    runtime_info: Optional[Dict[str, Any]] = None,\n",
        "    csv_path: str = \"Ollama models by size and call name (a limited list).csv\",\n",
        "    allow_paid_models: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Core router: choose a model and execute the call. Returns a dict with\n",
        "    selection details and the model output.\n",
        "    \"\"\"\n",
        "    catalog = build_catalog(csv_path, allow_paid_models)\n",
        "\n",
        "    complex_prompt = is_complex_prompt(prompt)\n",
        "    specialization = detect_specialization(prompt)\n",
        "    resources = snapshot_resources()\n",
        "\n",
        "    # Incorporate any runtime quotas into a tiny penalty (dummy)\n",
        "    quotas = (runtime_info or {}).get(\"api_quotas\", {})\n",
        "    openai_quota_low = quotas.get(\"openai_remaining\", 1000) < 50\n",
        "    ollama_quota_low = quotas.get(\"ollama_remaining\", 1000) < 10\n",
        "\n",
        "    # Score each candidate\n",
        "    scored = []\n",
        "    for spec in catalog:\n",
        "        s = score_candidate(spec,\n",
        "                            optimization=optimization,\n",
        "                            complex_prompt=complex_prompt,\n",
        "                            specialization=specialization,\n",
        "                            resources=resources)\n",
        "        # Quotas as penalties (illustrative)\n",
        "        if spec.provider == \"openai\" and openai_quota_low:\n",
        "            s += 1.0\n",
        "        if spec.provider == \"ollama\" and ollama_quota_low:\n",
        "            s += 0.5\n",
        "        scored.append((s, spec))\n",
        "\n",
        "    scored.sort(key=lambda x: x[0])\n",
        "    chosen = scored[0][1]\n",
        "\n",
        "    # Try to init and call; fall back if needed\n",
        "    error = None\n",
        "    response = None\n",
        "    try:\n",
        "        chat_model = init_model(chosen)\n",
        "        response = call_model(chat_model, prompt)\n",
        "    except Exception as e:\n",
        "        error = f\"{type(e).__name__}: {e}\"\n",
        "        # Fallback: attempt the next candidate\n",
        "        if len(scored) > 1:\n",
        "            try:\n",
        "                alt = scored[1][1]\n",
        "                chat_model = init_model(alt)\n",
        "                response = call_model(chat_model, prompt)\n",
        "                chosen = alt\n",
        "            except Exception as e2:\n",
        "                response = f\"[Router demo fallback] Could not reach any LLM. Last error: {type(e2).__name__}: {e2}\"\n",
        "\n",
        "    return {\n",
        "        \"chosen_model\": chosen.name,\n",
        "        \"provider\": chosen.provider,\n",
        "        \"notes\": chosen.notes,\n",
        "        \"complex_prompt\": complex_prompt,\n",
        "        \"specialization\": specialization,\n",
        "        \"optimization\": optimization,\n",
        "        \"resources\": resources,\n",
        "        \"error\": error,\n",
        "        \"response\": response,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YiKJAo7hfm5",
        "outputId": "33f39e92-bbff-42b7-ff2c-1789b809fc19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Ollama server...\n",
            "→ Ollama PID: 7355\n",
            "⏳ Waiting for Ollama to be ready…\n",
            "Ready!\n",
            "\n",
            "🚀 Pulling model 'llama3.2:1b'…\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "\n",
            "=== Router Decision ===\n",
            "Chosen: llama3.2:1b (provider=ollama)\n",
            "Notes:  Local, lightweight general model\n",
            "Complex prompt? False; specialization: None\n",
            "Optimization: speed\n",
            "Resources snapshot: {'cpu_percent': 0.0, 'available_gb': 10.75}\n",
            "\n",
            "=== Model Output (truncated) ===\n",
            "**Fibonacci Function and Time Complexity Analysis**\n",
            "=====================================================\n",
            "\n",
            "Here's a simple Python function to calculate the nth Fibonacci number using recursion:\n",
            "\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    \"\"\"\n",
            "    Returns the nth Fibonacci number.\n",
            "\n",
            "    Args:\n",
            "        n (int): The position of the Fibonacci number to return.\n",
            "\n",
            "    Returns:\n",
            "        int: The nth Fibonacci number.\n",
            "    \"\"\"\n",
            "    if n <= 0:\n",
            "        raise ValueError(\"n must be a positive integer\")\n",
            "    elif n == 1:\n",
            "        return 0\n",
            "    elif n == 2:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "```\n",
            "\n",
            "**Time Complexity Analysis**\n",
            "---------------------------\n",
            "\n",
            "The time complexity of this recursive function is O(2^n), which can be simplified to O(2^n). This is because each call to the function branches into two more calls, resulting in exponential growth.\n",
            "\n",
            "To illustrate this, consider a tree with n nodes. Each node has two children, so there are 2^k nodes for k = 1 to n. Therefore, the total number of nodes is 2^(n+1).\n",
            "\n",
            "In Python, the recursive function will be called approximately 2^n times, resulting in an exponential time complexity.\n",
            "\n",
            "**Optimization**\n",
            "----------------\n",
            "\n",
            "To improve pe\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# -----------------------\n",
        "# Demo entrypoint\n",
        "# -----------------------\n",
        "\n",
        "def main(prompt: str,\n",
        "      conversation_ctx: Optional[List[Dict[str, Any]]] = None,\n",
        "      optimization: str = \"speed\",   # 'speed' | 'cost' | 'depth'\n",
        "      runtime_info: Optional[Dict[str, Any]] = None,\n",
        "      allow_paid_models: bool = True):\n",
        "    # # Optional: start a background resource logger (1 minute, every 5s) — for demo visibility.\n",
        "    # # This is purely illustrative; comment out if not desired.\n",
        "    # try:\n",
        "    #     _ = start_resource_monitoring(logfile=\"resource_usage.log\", duration=60, interval=5)  # :contentReference[oaicite:5]{index=5}\n",
        "    # except Exception:\n",
        "    #     pass\n",
        "\n",
        "    result = route_and_dispatch(\n",
        "        prompt=prompt,\n",
        "        conversation_ctx=conversation_ctx,\n",
        "        optimization=optimization,\n",
        "        runtime_info=runtime_info,\n",
        "        allow_paid_models=allow_paid_models\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Router Decision ===\")\n",
        "    print(f\"Chosen: {result['chosen_model']} (provider={result['provider']})\")\n",
        "    print(f\"Notes:  {result['notes']}\")\n",
        "    print(f\"Complex prompt? {result['complex_prompt']}; specialization: {result['specialization']}\")\n",
        "    print(f\"Optimization: {result['optimization']}\")\n",
        "    print(f\"Resources snapshot: {result['resources']}\")\n",
        "    if result[\"error\"]:\n",
        "        print(f\"Init error (first try): {result['error']}\")\n",
        "\n",
        "    print(\"\\n=== Model Output (truncated) ===\")\n",
        "    print(str(result[\"response\"])[:1200])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample inputs (edit these to experiment)\n",
        "    prompt = (\n",
        "        \"Write a short Python function that returns the nth Fibonacci number, \"\n",
        "        \"then analyze the time complexity.\"\n",
        "    )\n",
        "    conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
        "    optimization = \"speed\"  # try 'speed', 'cost', or 'depth'\n",
        "\n",
        "    # Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
        "    runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
        "    allow_paid_models = True\n",
        "    main(prompt, conversation_ctx, optimization, runtime_info, allow_paid_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBq0T9sOYcgZ"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2yn_tVSVKj7"
      },
      "source": [
        "## Evaluating Various Scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RQt3rbGVQJK"
      },
      "source": [
        "### Optimize depth: coding task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQGgeSv0hAEF",
        "outputId": "990588f7-0d2c-40af-e1fa-c908a39c21ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Ollama server...\n",
            "→ Ollama PID: 7395\n",
            "⏳ Waiting for Ollama to be ready…\n",
            "Ready!\n",
            "\n",
            "🚀 Pulling model 'gpt-oss:20b'…\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "\n",
            "=== Router Decision ===\n",
            "Chosen: gpt-oss:20b (provider=ollama)\n",
            "Notes:  Local, medium size general model\n",
            "Complex prompt? False; specialization: None\n",
            "Optimization: depth\n",
            "Resources snapshot: {'cpu_percent': 35.0, 'available_gb': 10.74}\n",
            "\n",
            "=== Model Output (truncated) ===\n",
            "Here’s a compact, **iterative** implementation that returns the *n*‑th Fibonacci number (with the convention `fib(0)=0`, `fib(1)=1`):\n",
            "\n",
            "```python\n",
            "def fib(n: int) -> int:\n",
            "    \"\"\"Return the n‑th Fibonacci number (0‑based).\"\"\"\n",
            "    if n < 0:\n",
            "        raise ValueError(\"n must be non‑negative\")\n",
            "    a, b = 0, 1          # a = fib(0), b = fib(1)\n",
            "    for _ in range(n):\n",
            "        a, b = b, a + b  # shift one step forward\n",
            "    return a\n",
            "```\n",
            "\n",
            "### How it works\n",
            "* `a` always holds `fib(k)` and `b` holds `fib(k+1)` for the current loop index `k`.\n",
            "* Each iteration updates them to the next pair:  \n",
            "  `a ← fib(k+1)`, `b ← fib(k+2)`.\n",
            "* After `n` iterations, `a` equals `fib(n)`.\n",
            "\n",
            "### Time complexity\n",
            "The loop runs exactly `n` times, and each iteration does a constant amount of work (two integer assignments and one addition).  \n",
            "**Time complexity:** `O(n)`.\n",
            "\n",
            "### Space complexity\n",
            "Only a few integer variables are stored, regardless of `n`.  \n",
            "**Space complexity:** `O(1)`.\n",
            "\n",
            "---\n",
            "\n",
            "#### (Optional) Naïve recursive version\n",
            "\n",
            "```python\n",
            "def fib_recursive(n: int) -> int:\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    return fib_recursive(n-1) + fib_recursive(n-2)\n",
            "```\n",
            "\n",
            "*Time complexity:* `O(2^n)` (exponential) because the recursion tree\n"
          ]
        }
      ],
      "source": [
        "# Sample inputs (edit these to experiment)\n",
        "prompt = (\n",
        "    \"Write a short Python function that returns the nth Fibonacci number, \"\n",
        "    \"then analyze the time complexity.\"\n",
        ")\n",
        "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
        "optimization = \"depth\"  # try 'speed', 'cost', or 'depth'\n",
        "\n",
        "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
        "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
        "allow_paid_models = True\n",
        "main(prompt, conversation_ctx, optimization, runtime_info, allow_paid_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSLodW9kYLa0"
      },
      "source": [
        "### Optimize depth: coding task  \n",
        "Excluding paid remote models, sticking with local models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BoSrrj4YLa0",
        "outputId": "f7cf5da3-7974-418d-f8f4-fa92437c7636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Ollama server...\n",
            "→ Ollama PID: 8349\n",
            "⏳ Waiting for Ollama to be ready…\n",
            "Ready!\n",
            "\n",
            "🚀 Pulling model 'gpt-oss:20b'…\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "\n",
            "=== Router Decision ===\n",
            "Chosen: gpt-oss:20b (provider=ollama)\n",
            "Notes:  Local, medium size general model\n",
            "Complex prompt? False; specialization: None\n",
            "Optimization: depth\n",
            "Resources snapshot: {'cpu_percent': 58.5, 'available_gb': 9.63}\n",
            "\n",
            "=== Model Output (truncated) ===\n",
            "Here’s a compact, **iterative** implementation that returns the *n*‑th Fibonacci number (with the convention `fib(0)=0`, `fib(1)=1`):\n",
            "\n",
            "```python\n",
            "def fib(n: int) -> int:\n",
            "    \"\"\"Return the n‑th Fibonacci number (0‑based).\"\"\"\n",
            "    if n < 0:\n",
            "        raise ValueError(\"n must be non‑negative\")\n",
            "    a, b = 0, 1          # a = fib(0), b = fib(1)\n",
            "    for _ in range(n):\n",
            "        a, b = b, a + b  # shift one step forward\n",
            "    return a\n",
            "```\n",
            "\n",
            "### How it works\n",
            "* `a` always holds the current Fibonacci number `fib(k)`.\n",
            "* `b` holds the next one `fib(k+1)`.\n",
            "* Each loop iteration moves one step forward: `a` becomes `b`, and `b` becomes `a + b` (the next Fibonacci number).\n",
            "\n",
            "### Time complexity\n",
            "The loop runs exactly **n** times, and each iteration does a constant amount of work (two assignments and one addition).  \n",
            "Therefore:\n",
            "\n",
            "```\n",
            "T(n) = Θ(n)\n",
            "```\n",
            "\n",
            "So the algorithm is linear in the index *n*.\n",
            "\n",
            "### Space complexity\n",
            "Only a few integer variables are stored, regardless of *n*:\n",
            "\n",
            "```\n",
            "S(n) = Θ(1)\n",
            "```\n",
            "\n",
            "### (Optional) Naïve recursive version\n",
            "For completeness, here’s the classic recursive definition:\n",
            "\n",
            "```python\n",
            "def fib_recursive(n: int) -> int:\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    return fib_recursive(n-1) + fib_recurs\n"
          ]
        }
      ],
      "source": [
        "# Sample inputs (edit these to experiment)\n",
        "prompt = (\n",
        "    \"Write a short Python function that returns the nth Fibonacci number, \"\n",
        "    \"then analyze the time complexity.\"\n",
        ")\n",
        "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
        "optimization = \"depth\"  # try 'speed', 'cost', or 'depth'\n",
        "\n",
        "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
        "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
        "allow_paid_models = False\n",
        "main(prompt, conversation_ctx, optimization, runtime_info, allow_paid_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD2QkkflXpR7"
      },
      "source": [
        "### Optimize cost: Math puzzle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVjmIH5-XpR7",
        "outputId": "6ff3e67b-25e2-474c-99c7-319e73cd6e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Ollama server...\n",
            "→ Ollama PID: 14513\n",
            "⏳ Waiting for Ollama to be ready…\n",
            "Ready!\n",
            "\n",
            "🚀 Pulling model 'gpt-oss:20b'…\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "\n",
            "=== Router Decision ===\n",
            "Chosen: gpt-oss:20b (provider=ollama)\n",
            "Notes:  Local, medium size general model\n",
            "Complex prompt? True; specialization: math\n",
            "Optimization: cost\n",
            "Resources snapshot: {'cpu_percent': 0.0, 'available_gb': 11.1}\n",
            "\n",
            "=== Model Output (truncated) ===\n",
            "**Proof that \\(\\sqrt{2}\\) is irrational**\n",
            "\n",
            "Assume, for contradiction, that \\(\\sqrt{2}\\) can be written as a fraction of two integers in lowest terms:\n",
            "\\[\n",
            "\\sqrt{2}=\\frac{p}{q}\\qquad (p,q\\in\\mathbb Z,\\; \\gcd(p,q)=1,\\; q\\neq 0).\n",
            "\\]\n",
            "\n",
            "Squaring both sides gives\n",
            "\\[\n",
            "2=\\frac{p^{2}}{q^{2}}\\quad\\Longrightarrow\\quad p^{2}=2q^{2}.\n",
            "\\]\n",
            "\n",
            "Thus \\(p^{2}\\) is even, so \\(p\\) itself must be even (the square of an odd number is odd).  \n",
            "Write \\(p=2k\\) for some integer \\(k\\). Substituting back:\n",
            "\\[\n",
            "(2k)^{2}=2q^{2}\\quad\\Longrightarrow\\quad 4k^{2}=2q^{2}\\quad\\Longrightarrow\\quad q^{2}=2k^{2}.\n",
            "\\]\n",
            "\n",
            "Now \\(q^{2}\\) is even, so \\(q\\) is even as well.  \n",
            "But if both \\(p\\) and \\(q\\) are even, they share a common factor of 2, contradicting the assumption that \\(\\frac{p}{q}\\) was in lowest terms.\n",
            "\n",
            "Hence our assumption is false, and \\(\\sqrt{2}\\) cannot be expressed as a ratio of two integers—it is irrational.\n",
            "\n",
            "---\n",
            "\n",
            "**One‑sentence explanation for a 6‑year‑old**\n",
            "\n",
            "\"The number that, when you multiply it by itself, gives 2 can’t be written as a simple fraction, so it’s a special number that can’t be made from whole numbers.\"\n"
          ]
        }
      ],
      "source": [
        "# Sample inputs (edit these to experiment)\n",
        "prompt = (\n",
        "    \"Prove that the squary root of 2 is irrational.\"\n",
        "    \"Then simplify it in one sentence for a 6 year old.\"\n",
        ")\n",
        "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
        "optimization = \"cost\"  # try 'speed', 'cost', or 'depth'\n",
        "\n",
        "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
        "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
        "allow_paid_models = True\n",
        "main(prompt, conversation_ctx, optimization, runtime_info, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b20lrxUebTPz"
      },
      "source": [
        "### Optimize speed: Math puzzle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ48RfhDbTP0",
        "outputId": "cf0ac745-3e43-41f3-ea28-7bc2d9349e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Setting up remote OpenAI chat model…\n",
            "All done setting up OpenAI (ChatOpenAI).\n",
            "\n",
            "\n",
            "=== Router Decision ===\n",
            "Chosen: gpt-4o (provider=openai)\n",
            "Notes:  Remote, strong reasoning\n",
            "Complex prompt? True; specialization: math\n",
            "Optimization: speed\n",
            "Resources snapshot: {'cpu_percent': 0.0, 'available_gb': 9.64}\n",
            "\n",
            "=== Model Output (truncated) ===\n",
            "To prove that the square root of 2 is irrational, we can use a proof by contradiction. Assume that the square root of 2 is rational, meaning it can be expressed as a fraction \\( \\frac{a}{b} \\) where \\( a \\) and \\( b \\) are integers with no common factors other than 1, and \\( b \\neq 0 \\). Then we have:\n",
            "\n",
            "\\[\n",
            "\\sqrt{2} = \\frac{a}{b}\n",
            "\\]\n",
            "\n",
            "Squaring both sides gives:\n",
            "\n",
            "\\[\n",
            "2 = \\frac{a^2}{b^2}\n",
            "\\]\n",
            "\n",
            "Multiplying both sides by \\( b^2 \\) gives:\n",
            "\n",
            "\\[\n",
            "2b^2 = a^2\n",
            "\\]\n",
            "\n",
            "This implies that \\( a^2 \\) is even, which means \\( a \\) must also be even (since the square of an odd number is odd). So, let \\( a = 2k \\) for some integer \\( k \\). Substituting back, we get:\n",
            "\n",
            "\\[\n",
            "2b^2 = (2k)^2 = 4k^2\n",
            "\\]\n",
            "\n",
            "Dividing both sides by 2 gives:\n",
            "\n",
            "\\[\n",
            "b^2 = 2k^2\n",
            "\\]\n",
            "\n",
            "This implies that \\( b^2 \\) is even, so \\( b \\) must also be even. However, this contradicts our original assumption that \\( a \\) and \\( b \\) have no common factors other than 1, because both being even means they have a common factor of 2. Therefore, our assumption that \\( \\sqrt{2} \\) is rational must be false, so \\( \\sqrt{2} \\) is irrational.\n",
            "\n",
            "For a 6-year-old: \"You can't write the square root of 2 as a simple fraction, no matter how hard you try!\"\n"
          ]
        }
      ],
      "source": [
        "# Sample inputs (edit these to experiment)\n",
        "prompt = (\n",
        "    \"Prove that the squary root of 2 is irrational.\"\n",
        "    \"Then simplify it in one sentence for a 6 year old.\"\n",
        ")\n",
        "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
        "optimization = \"speed\"  # try 'speed', 'cost', or 'depth'\n",
        "\n",
        "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
        "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
        "allow_paid_models = True\n",
        "main(prompt, conversation_ctx, optimization, runtime_info, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FWsmYA5bTlE"
      },
      "source": [
        "### Optimize speed: Math puzzle\n",
        "Excluding paid remote models, sticking with local models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA0-gLSibTlE",
        "outputId": "15400a11-1e76-43f8-dfd8-1ca426f75ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Ollama server...\n",
            "→ Ollama PID: 15046\n",
            "⏳ Waiting for Ollama to be ready…\n",
            "Ready!\n",
            "\n",
            "🚀 Pulling model 'gpt-oss:20b'…\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "\n",
            "=== Router Decision ===\n",
            "Chosen: gpt-oss:20b (provider=ollama)\n",
            "Notes:  Local, medium size general model\n",
            "Complex prompt? True; specialization: math\n",
            "Optimization: cost\n",
            "Resources snapshot: {'cpu_percent': 2.5, 'available_gb': 9.64}\n",
            "\n",
            "=== Model Output (truncated) ===\n",
            "**Proof that \\(\\sqrt{2}\\) is irrational**\n",
            "\n",
            "Assume, for contradiction, that \\(\\sqrt{2}\\) can be written as a fraction of two integers in lowest terms:\n",
            "\\[\n",
            "\\sqrt{2}=\\frac{p}{q}\\qquad (p,q\\in\\mathbb Z,\\; \\gcd(p,q)=1,\\; q\\neq 0).\n",
            "\\]\n",
            "\n",
            "Squaring both sides gives\n",
            "\\[\n",
            "2=\\frac{p^{2}}{q^{2}}\\quad\\Longrightarrow\\quad p^{2}=2q^{2}.\n",
            "\\]\n",
            "\n",
            "Thus \\(p^{2}\\) is even, so \\(p\\) itself must be even (the square of an odd number is odd).  \n",
            "Write \\(p=2k\\) for some integer \\(k\\). Substituting back:\n",
            "\\[\n",
            "(2k)^{2}=2q^{2}\\quad\\Longrightarrow\\quad 4k^{2}=2q^{2}\\quad\\Longrightarrow\\quad q^{2}=2k^{2}.\n",
            "\\]\n",
            "\n",
            "Now \\(q^{2}\\) is even, so \\(q\\) is even as well.  \n",
            "But if both \\(p\\) and \\(q\\) are even, they share a common factor of 2, contradicting the assumption that \\(\\frac{p}{q}\\) was in lowest terms.\n",
            "\n",
            "Hence our assumption is false, and \\(\\sqrt{2}\\) cannot be expressed as a ratio of two integers—it is irrational.\n",
            "\n",
            "---\n",
            "\n",
            "**One‑sentence explanation for a 6‑year‑old**\n",
            "\n",
            "\"The number that, when you multiply it by itself, gives 2 can’t be written as a simple fraction, so it’s a special number that can’t be made from whole numbers.\"\n"
          ]
        }
      ],
      "source": [
        "# Sample inputs (edit these to experiment)\n",
        "prompt = (\n",
        "    \"Prove that the squary root of 2 is irrational.\"\n",
        "    \"Then simplify it in one sentence for a 6 year old.\"\n",
        ")\n",
        "conversation_ctx = [{\"role\": \"system\", \"content\": \"You are concise.\"}]\n",
        "optimization = \"cost\"  # try 'speed', 'cost', or 'depth'\n",
        "\n",
        "# Dummy runtime info: pretend OpenAI quota is healthy; Ollama is unlimited.\n",
        "runtime_info = {\"api_quotas\": {\"openai_remaining\": 500, \"ollama_remaining\": 9999}}\n",
        "allow_paid_models = False\n",
        "main(prompt, conversation_ctx, optimization, runtime_info, )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "948vnVL3fCJU",
        "0Akit5rDfOtR"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}